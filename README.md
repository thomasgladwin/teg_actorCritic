# teg_actorCritic
Actor-critic and SARSA(lambda) models with a toy-environment.

Practice project for Sutton & Barto. The models have linear parameterizations for the agent's weights. The code uses separate classes to make the separation of information between elements clear. For the actor-critic model, these are: the Environment (which responds to actions), the Critic (which receives rewards from the Environment and bootstrap-learns state-value functions) and the Actor (which receives Temporal Difference signals from the critic and bootstrap-learns state-action preferences). The SARSA model has a single class for the agent. There's also a Simulation class for convenience.

In principle, the SARSA, Actor-Critic and Simulation classes should be pretty general-purpose. The Environment just has to receive actions and produce rewards to work with them, and it contains all the specific machinery connecting actions to rewards.

In this case, the environment is a 2D GridWorld, with a starting point and a terminal point that must be reached in as few steps as possible, ending the episode. Starting and terminal ppoints can be random per episode. There are vertical crosswinds that blow the agent off-course with varying speeds, walls which cannot be moved into, and pits with an added negative reward when moved into. Pits can be preset or randomized. The Environment has options to determine what features are observable: the actual grid coordinates, the neighbouring pits, and the relative direction of the terminal point. The latter two features are for use with random grids, and let the agent learn to move towards a varying terminal point while avoiding randomly located pits.
